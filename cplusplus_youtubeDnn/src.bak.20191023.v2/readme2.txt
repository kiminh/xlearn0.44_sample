src->solver->train_main.cc

==========================================================
cd demo/classification/kuwo
scp 10.0.34.12:/data/ming.li/workspace/TrainModel/userdb/userPlayMusicSeq/DataSet/20190916/cplusplus/ctr_dateset.youtubeDnn.txt ./
scp 10.0.34.12:/data/ming.li/workspace/TrainModel/userdb/userPlayMusicSeq/DataSet/20190916/cplusplus/init.modle.youtubeDnn.txt ./


programe arg.. =
working dir.. =
/data/ming.li/workspace/TrainModel/api/fm/cplusplus_youtubeDnn/demo/classification/kuwo

run->edit-config..->Allaplication->






======================================================
==== one thread
======================================================
./ctr_dateset.youtubeDnn.train.sample.v1.txt -v ./ctr_dateset.youtubeDnn.test.sample.v1.txt -s 0 -e 200 -nthread 1 -k 100 -x acc -p sgd -r 0.1 -sw 100
>> >>
[------------] Epoch      Train log_loss       Test log_loss       Test Accuarcy     Time cost (sec)
[    0%      ]     1            0.685068            0.684631            0.524283              355.85
[    1%      ]     2            0.682797            0.684385            0.524650              358.09


./ctr_dateset.youtubeDnn.train.sample.v1.txt -v ./ctr_dateset.youtubeDnn.test.sample.v1.txt -s 0 -e 200 -nthread 1 -k 100 -x acc -p sgd -r 0.01 -pre ./init.modle.youtubeDnn.txt -sw 100
>> >> gcc: O0 -std=c++11
>> >> index_t fullLayer_Cells[3]={512,100};
>> >> batch_size=8
>> >> isjustloadembedding=true     //表示仅仅初始化embedding  用于第一次训练
>> >> nthread =1
>> >> update use UpDate_AllThreads functions
>> >> the real batch_size is batch_size*nthread=8.0*1
[------------] Epoch      Train log_loss       Test log_loss       Test Accuarcy     Time cost (sec)
[    0%      ]     1            0.450146            0.437620            0.853324              265.61
[    1%      ]     2            0.433854            0.431830            0.856880              265.85
[    1%      ]     3            0.426587            0.429014            0.855654              266.34
[    2%      ]     4            0.421310            0.427691            0.856267              263.70
[    2%      ]     5            0.416807            0.427014            0.855040              266.43
[    3%      ]     6            0.412779            0.426589            0.853446              262.95
[    3%      ]     7            0.409104            0.426648            0.851974              268.55
[    4%      ]     8            0.405573            0.426658            0.851239              264.91
[    4%      ]     9            0.402153            0.426716            0.850993              264.89
[    5%      ]    10            0.398854            0.427216            0.848908              265.36
[    5%      ]    11            0.395609            0.427575            0.846824              263.32





======================================================
==== mult threads
======================================================
./ctr_dateset.youtubeDnn.train.sample.v1.txt -v ./ctr_dateset.youtubeDnn.test.sample.v1.txt -s 0 -e 200 -nthread 8 -k 100 -x acc -p sgd -r 0.1 -pre ./init.modle.youtubeDnn.txt -sw 100
>> >> gcc: O0 -std=c++11
>> >> index_t fullLayer_Cells[3]={512,100};
>> >> batch_size=8
>> >> isjustloadembedding=true     //表示仅仅初始化embedding  用于第一次训练
>> >> nthread =8
>> >> update use UpDate_AllThreads functions
>> >> the real batch_size is batch_size*nthread=64
[------------] Epoch      Train log_loss       Test log_loss       Test Accuarcy     Time cost (sec)
[    0%      ]     1            0.449169            0.438987            0.850620               68.96
[    1%      ]     2            0.432812            0.435440            0.849540               69.39
[    1%      ]     3            0.425315            0.432414            0.849950               68.58
>> >> gcc: -O3 -std=c++11
>> >> index_t fullLayer_Cells[3]={512,100};
>> >> batch_size=8
>> >> isjustloadembedding=true     //表示仅仅初始化embedding  用于第一次训练
>> >> nthread =8
>> >> update use UpDate_AllThreads functions
>> >> the real batch_size is batch_size*nthread=64
[------------] Epoch      Train log_loss       Test log_loss       Test Accuarcy     Time cost (sec)
[    0%      ]     1            0.449196            0.440565            0.846010               12.34
[    1%      ]     2            0.432978            0.433727            0.857790               12.12
[    1%      ]     3            0.425898            0.430925            0.857040               11.87
[    2%      ]     4            0.420225            0.430244            0.852730               11.83
[    2%      ]     5            0.415521            0.430590            0.850720               12.05
[    3%      ]     6            0.411512            0.432128            0.847390               12.23
[    3%      ]     7            0.407457            0.430664            0.853440               11.86
[    4%      ]     8            0.403652            0.430168            0.854440               11.97
[    4%      ]     9            0.399903            0.431369            0.845280               12.02
[    5%      ]    10            0.395914            0.430647            0.848650               11.99
[    5%      ]    11            0.392610            0.431772            0.848080               11.90
[    6%      ]    12            0.389039            0.433298            0.844110               12.04
[    6%      ]    13            0.385873            0.434156            0.850290               12.01
[    7%      ]    14            0.382077            0.434432            0.842140               12.17
[    7%      ]    15            0.378586            0.436682            0.840120               11.96
[    8%      ]    16            0.374796            0.436041            0.841690               11.73
[    8%      ]    17            0.371260            0.438576            0.841640               11.76
[    9%      ]    18            0.368037            0.446394            0.832590               11.94
[    9%      ]    19            0.364082            0.442383            0.834050               11.85
[   10%      ]    20            0.360524            0.444742            0.834920               11.90
[   10%      ]    21            0.356644            0.447314            0.831620               12.23
[   11%      ]    22            0.353138            0.449143            0.830390               11.89
[   11%      ]    23            0.349267            0.453133            0.826350               11.90
[   12%      ]    24            0.345167            0.452399            0.825990               11.80
[   12%      ]    25            0.341661            0.456015            0.825460               12.10
[   13%      ]    26            0.337445            0.457091            0.825160               11.97
[   13%      ]    27            0.333585            0.457378            0.824070               12.11
[   14%      ]    28            0.329990            0.464675            0.821070               12.03
[   14%      ]    29            0.325786            0.466515            0.821400               11.83
[   15%      ]    30            0.321850            0.468511            0.817550               11.98
[   15%      ]    31            0.317569            0.469972            0.817080               12.01
[   16%      ]    32            0.313996            0.476685            0.814700               12.05
[   16%      ]    33            0.309955            0.478968            0.811750               11.93
[   17%      ]    34            0.306166            0.480783            0.811500               12.13
[   17%      ]    35            0.302397            0.485879            0.808640               12.13
[   18%      ]    36            0.297857            0.488623            0.805470               11.96
[   18%      ]    37            0.293915            0.492029            0.805340               11.96
[   19%      ]    38            0.290340            0.496579            0.803300               12.04
[   19%      ]    39            0.285238            0.497776            0.805430               11.96
[   20%      ]    40            0.282451            0.501092            0.802850               12.15
[   20%      ]    41            0.277673            0.504762            0.800660               12.33
[   21%      ]    42            0.273569            0.508644            0.799120               11.84
[   21%      ]    43            0.269323            0.511564            0.797320               12.40
[   22%      ]    44            0.265768            0.516178            0.799440               12.05
[   22%      ]    45            0.262049            0.517794            0.796480               12.49
[   23%      ]    46            0.257786            0.528805            0.797250               11.76
[   23%      ]    47            0.254065            0.536663            0.786070               12.06
[   24%      ]    48            0.249848            0.533868            0.791290               11.78
[   24%      ]    49            0.247078            0.544575            0.788690               12.05
[   25%      ]    50            0.242893            0.542934            0.793440               12.03
[   25%      ]    51            0.239055            0.554174            0.789540               12.10
[   26%      ]    52            0.235640            0.548795            0.789580               11.98
[   26%      ]    53            0.231689            0.555522            0.790100               11.86
[   27%      ]    54            0.228480            0.567825            0.783140               12.21
[   27%      ]    55            0.225518            0.571982            0.784590               12.04
[   28%      ]    56            0.222251            0.575331            0.785500               12.01
[   28%      ]    57            0.218212            0.578895            0.786410               12.01
[   28%      ]    58            0.214708            0.580860            0.781860               12.27
[   29%      ]    59            0.211994            0.584169            0.785510               12.15
[   30%      ]    60            0.208392            0.597564            0.774060               12.07
[   30%      ]    61            0.206895            0.618594            0.755550               11.88
[   31%      ]    62            0.201577            0.605573            0.778160               11.91
[   31%      ]    63            0.198913            0.599613            0.777470               12.03
[   32%      ]    64            0.196203            0.612107            0.777950               11.86
[   32%      ]    65            0.192849            0.610946            0.779310               12.11
[   33%      ]    66            0.189792            0.633560            0.776790               11.95
[   33%      ]    67            0.186059            0.622804            0.775340               12.24
[   34%      ]    68            0.182688            0.633227            0.769770               12.08
[   34%      ]    69            0.181227            0.645179            0.774660               12.12
[   35%      ]    70            0.178888            0.639627            0.768910               12.13
[   35%      ]    71            0.175163            0.653164            0.772540               11.81
[   36%      ]    72            0.173470            0.645879            0.773490               12.38



./ctr_dateset.youtubeDnn.train.v1.txt -v ./ctr_dateset.youtubeDnn.test.sample.v1.txt -s 0 -e 5 -nthread 8 -k 100 -x acc -p sgd -r 0.1 -pre ./init.modle.youtubeDnn.txt -sw 100
>>

others labs
===================
./ctr_dateset.youtubeDnn.train.v1.txt -v ./ctr_dateset.youtubeDnn.test.v1.txt -s 0 -e 200 -nthread 1 -k 100 -x acc -p sgd -r 0.1 -pre ./init.modle.youtubeDnn.txt -sw 100

./ctr_dateset.youtubeDnn.train.txt -v ./ctr_dateset.youtubeDnn.test.txt -s 0 -e 200 -nthread 8 -k 100 -x acc -p sgd -r 0.001 -pre ./init.modle.youtubeDnn.txt  -sw 100





